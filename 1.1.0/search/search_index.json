{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Spark Native SQL Engine A Native Engine for Spark SQL with vectorized SIMD optimizations Introduction Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD optimized kernels and LLVM based SQL engine Gandiva are also very efficient. Native SQL Engine used these technologies and brought better performance to Spark SQL. Key Features Apache Arrow formatted intermediate data among Spark operator With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format. Apache Arrow based Native Readers for Parquet and other formats A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source Apache Arrow Compute/Gandiva based operators We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch. Native Columnar Shuffle Operator with efficient compression support We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format. Please check the operator supporting details here Build the Plugin Building by Conda If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-columnar-core-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Get Started . Building by yourself If you prefer to build from the source code on your hand, please follow below steps to set up your environment. Prerequisite There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. If you are running a SPARK Cluster, please make sure all the software are installed in every single node. Installation Please check the document Installation Guide Configuration & Testing Please check the document Configuration Guide Get started To enable OAP NativeSQL Engine, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar should be added to Spark configuration. We also recommend to use spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar . We will demonstrate an example by using both jar files. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if native sql engine works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/date_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should show up on Spark console and you can check the DAG diagram with some Columnar Processing stage. Native SQL engine still lacks some features, please check out the limitations . Performance data For advanced performance testing, below charts show the results by using two benchmarks: 1. Decision Support Benchmark1 and 2. Decision Support Benchmark2. All the testing environment for Decision Support Benchmark1&2 are using 1 master + 3 workers and Intel(r) Xeon(r) Gold 6252 CPU|384GB memory|NVMe SSD x3 per single node with 1.5TB dataset. * Decision Support Benchmark1 is a query set modified from TPC-H benchmark . We change Decimal to Double since Decimal hasn't been supported in OAP v1.0-Native SQL Engine. Overall, the result shows a 1.49X performance speed up from OAP v1.0-Native SQL Engine comparing to Vanilla SPARK 3.0.0. We also put the detail result by queries, most of queries in Decision Support Benchmark1 can take the advantages from Native SQL Engine. The performance boost ratio may depend on the individual query. Decision Support Benchmark2 is a query set modified from TPC-DS benchmark . We change Decimal to Doubel since Decimal hasn't been supported in OAP v1.0-Native SQL Engine. We pick up 10 queries which can be fully supported in OAP v1.0-Native SQL Engine and the result shows a 1.26X performance speed up comparing to Vanilla SPARK 3.0.0. Please notes the performance data is not an official from TPC-H and TPC-DS. The actual performance result may vary by individual workloads. Please try your workloads with native SQL Engine first and check the DAG or log file to see if all the operators can be supported in OAP-Native SQL Engine. Coding Style For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details. Contact chendi.xue@intel.com binwei.yang@intel.com","title":"Spark Native SQL Engine"},{"location":"#spark-native-sql-engine","text":"A Native Engine for Spark SQL with vectorized SIMD optimizations","title":"Spark Native SQL Engine"},{"location":"#introduction","text":"Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD optimized kernels and LLVM based SQL engine Gandiva are also very efficient. Native SQL Engine used these technologies and brought better performance to Spark SQL.","title":"Introduction"},{"location":"#key-features","text":"","title":"Key Features"},{"location":"#apache-arrow-formatted-intermediate-data-among-spark-operator","text":"With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format.","title":"Apache Arrow formatted intermediate data among Spark operator"},{"location":"#apache-arrow-based-native-readers-for-parquet-and-other-formats","text":"A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source","title":"Apache Arrow based Native Readers for Parquet and other formats"},{"location":"#apache-arrow-computegandiva-based-operators","text":"We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch.","title":"Apache Arrow Compute/Gandiva based operators"},{"location":"#native-columnar-shuffle-operator-with-efficient-compression-support","text":"We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format. Please check the operator supporting details here","title":"Native Columnar Shuffle Operator with efficient compression support"},{"location":"#build-the-plugin","text":"","title":"Build the Plugin"},{"location":"#building-by-conda","text":"If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-columnar-core-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Get Started .","title":"Building by Conda"},{"location":"#building-by-yourself","text":"If you prefer to build from the source code on your hand, please follow below steps to set up your environment.","title":"Building by yourself"},{"location":"#prerequisite","text":"There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. If you are running a SPARK Cluster, please make sure all the software are installed in every single node.","title":"Prerequisite"},{"location":"#installation","text":"Please check the document Installation Guide","title":"Installation"},{"location":"#configuration-testing","text":"Please check the document Configuration Guide","title":"Configuration &amp; Testing"},{"location":"#get-started","text":"To enable OAP NativeSQL Engine, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar should be added to Spark configuration. We also recommend to use spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar . We will demonstrate an example by using both jar files. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if native sql engine works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/date_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should show up on Spark console and you can check the DAG diagram with some Columnar Processing stage. Native SQL engine still lacks some features, please check out the limitations .","title":"Get started"},{"location":"#performance-data","text":"For advanced performance testing, below charts show the results by using two benchmarks: 1. Decision Support Benchmark1 and 2. Decision Support Benchmark2. All the testing environment for Decision Support Benchmark1&2 are using 1 master + 3 workers and Intel(r) Xeon(r) Gold 6252 CPU|384GB memory|NVMe SSD x3 per single node with 1.5TB dataset. * Decision Support Benchmark1 is a query set modified from TPC-H benchmark . We change Decimal to Double since Decimal hasn't been supported in OAP v1.0-Native SQL Engine. Overall, the result shows a 1.49X performance speed up from OAP v1.0-Native SQL Engine comparing to Vanilla SPARK 3.0.0. We also put the detail result by queries, most of queries in Decision Support Benchmark1 can take the advantages from Native SQL Engine. The performance boost ratio may depend on the individual query. Decision Support Benchmark2 is a query set modified from TPC-DS benchmark . We change Decimal to Doubel since Decimal hasn't been supported in OAP v1.0-Native SQL Engine. We pick up 10 queries which can be fully supported in OAP v1.0-Native SQL Engine and the result shows a 1.26X performance speed up comparing to Vanilla SPARK 3.0.0. Please notes the performance data is not an official from TPC-H and TPC-DS. The actual performance result may vary by individual workloads. Please try your workloads with native SQL Engine first and check the DAG or log file to see if all the operators can be supported in OAP-Native SQL Engine.","title":"Performance data"},{"location":"#coding-style","text":"For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details.","title":"Coding Style"},{"location":"#contact","text":"chendi.xue@intel.com binwei.yang@intel.com","title":"Contact"},{"location":"ApacheArrowInstallation/","text":"llvm-7.0: Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install cmake: Please make sure your cmake version is qualified based on the prerequisite. Arrow git clone https://github.com/oap-project/arrow.git cd arrow && git checkout arrow-3.0.0-oap-1.1 mkdir -p arrow/cpp/release-build cd arrow/cpp/release-build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_CSV=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make -j make install # build java cd ../../java # change property 'arrow.cpp.build.dir' to the relative path of cpp build dir in gandiva/pom.xml mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests # if you are behine proxy, please also add proxy for socks mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests -DsocksProxyHost=${proxyHost} -DsocksProxyPort=1080 run test mvn test -pl adapter/parquet -P arrow-jni mvn test -pl gandiva -P arrow-jni After arrow installed in the specific directory, please make sure to set up -Dbuild_arrow=OFF -Darrow_root=/path/to/arrow when building Native SQL Engine.","title":"ApacheArrowInstallation"},{"location":"ApacheArrowInstallation/#llvm-70","text":"Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install","title":"llvm-7.0:"},{"location":"ApacheArrowInstallation/#cmake","text":"Please make sure your cmake version is qualified based on the prerequisite.","title":"cmake:"},{"location":"ApacheArrowInstallation/#arrow","text":"git clone https://github.com/oap-project/arrow.git cd arrow && git checkout arrow-3.0.0-oap-1.1 mkdir -p arrow/cpp/release-build cd arrow/cpp/release-build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_CSV=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make -j make install # build java cd ../../java # change property 'arrow.cpp.build.dir' to the relative path of cpp build dir in gandiva/pom.xml mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests # if you are behine proxy, please also add proxy for socks mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests -DsocksProxyHost=${proxyHost} -DsocksProxyPort=1080 run test mvn test -pl adapter/parquet -P arrow-jni mvn test -pl gandiva -P arrow-jni After arrow installed in the specific directory, please make sure to set up -Dbuild_arrow=OFF -Darrow_root=/path/to/arrow when building Native SQL Engine.","title":"Arrow"},{"location":"Configuration/","text":"Spark Configurations for Native SQL Engine There are many configuration could impact the Native SQL Engine performance and can be fine tune in Spark. You can add these configuration into spark-defaults.conf to enable or disable the setting. Parameters Description Recommend Setting spark.driver.extraClassPath To add Arrow Data Source and Native SQL Engine jar file in Spark Driver /path/to/jar_file1:/path/to/jar_file2 spark.executor.extraClassPath To add Arrow Data Source and Native SQL Engine jar file in Spark Executor /path/to/jar_file1:/path/to/jar_file2 spark.executorEnv.LIBARROW_DIR To set up the location of Arrow library, by default it will search the loation of jar to be uncompressed /path/to/arrow_library/ spark.executorEnv.CC To set up the location of gcc /path/to/gcc/ spark.executor.memory To set up how much memory to be used for Spark Executor. spark.memory.offHeap.size To set up how much memory to be used for Java OffHeap. Please notice Native SQL Engine will leverage this setting to allocate memory space for native usage even offHeap is disabled. The value is based on your system and it is recommended to set it larger if you are facing Out of Memory issue in Native SQL Engine 30G spark.executor.extraJavaOptions To set up how much Direct Memory to be used for Native SQL Engine. The value is based on your system and it is recommended to set it larger if you are facing Out of Memory issue in Native SQL Engine -XX:MaxDirectMemorySize=30G spark.sql.sources.useV1SourceList Choose to use V1 source avro spark.sql.join.preferSortMergeJoin To turn off preferSortMergeJoin in Spark false spark.sql.extensions To turn on Native SQL Engine Plugin com.intel.oap.ColumnarPlugin spark.shuffle.manager To turn on Native SQL Engine Columnar Shuffle Plugin org.apache.spark.shuffle.sort.ColumnarShuffleManager spark.oap.sql.columnar.batchscan Enable or Disable Columnar Batchscan, default is true true spark.oap.sql.columnar.hashagg Enable or Disable Columnar Hash Aggregate, default is true true spark.oap.sql.columnar.projfilter Enable or Disable Columnar Project and Filter, default is true true spark.oap.sql.columnar.codegen.sort Enable or Disable Columnar Sort, default is true true spark.oap.sql.columnar.window Enable or Disable Columnar Window, default is true true spark.oap.sql.columnar.shuffledhashjoin Enable or Disable ShffuledHashJoin, default is true true spark.oap.sql.columnar.sortmergejoin Enable or Disable Columnar Sort Merge Join, default is true true spark.oap.sql.columnar.union Enable or Disable Columnar Union, default is true true spark.oap.sql.columnar.expand Enable or Disable Columnar Expand, default is true true spark.oap.sql.columnar.broadcastexchange Enable or Disable Columnar Broadcast Exchange, default is true true spark.oap.sql.columnar.nanCheck Enable or Disable Nan Check, default is true true spark.oap.sql.columnar.hashCompare Enable or Disable Hash Compare in HashJoins or HashAgg, default is true true spark.oap.sql.columnar.broadcastJoin Enable or Disable Columnar BradcastHashJoin, default is true true spark.oap.sql.columnar.wholestagecodegen Enable or Disable Columnar WholeStageCodeGen, default is true true spark.oap.sql.columnar.preferColumnar Enable or Disable Columnar Operators, default is false. This parameter could impact the performance in different case. In some cases, to set false can get some performance boost. false spark.oap.sql.columnar.joinOptimizationLevel Fallback to row operators if there are several continous joins 6 spark.sql.execution.arrow.maxRecordsPerBatch Set up the Max Records per Batch 10000 spark.oap.sql.columnar.wholestagecodegen.breakdownTime Enable or Disable metrics in Columnar WholeStageCodeGen false spark.oap.sql.columnar.tmp_dir Set up a folder to store the codegen files /tmp spark.oap.sql.columnar.shuffle.customizedCompression.codec Set up the codec to be used for Columnar Shuffle, default is lz4 lz4 spark.oap.sql.columnar.numaBinding Set up NUMABinding, default is false true spark.oap.sql.columnar.coreRange Set up the core range for NUMABinding, only works when numaBinding set to true. The setting is based on the number of cores in your system. Use 72 cores as an example. 0-17,36-53 |18-35,54-71 Below is an example for spark-default.conf, if you are using conda to install OAP project. ##### Columnar Process Configuration spark.sql.sources.useV1SourceList avro spark.sql.join.preferSortMergeJoin false spark.sql.extensions com.intel.oap.ColumnarPlugin spark.shuffle.manager org.apache.spark.shuffle.sort.ColumnarShuffleManager # note native sql engine depends on arrow data source spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executorEnv.LIBARROW_DIR $HOME/miniconda2/envs/oapenv spark.executorEnv.CC $HOME/miniconda2/envs/oapenv/bin/gcc ###### Before you start spark, you must use below command to add some environment variables. export CC=$HOME/miniconda2/envs/oapenv/bin/gcc export LIBARROW_DIR=$HOME/miniconda2/envs/oapenv/","title":"Configuration"},{"location":"Configuration/#spark-configurations-for-native-sql-engine","text":"There are many configuration could impact the Native SQL Engine performance and can be fine tune in Spark. You can add these configuration into spark-defaults.conf to enable or disable the setting. Parameters Description Recommend Setting spark.driver.extraClassPath To add Arrow Data Source and Native SQL Engine jar file in Spark Driver /path/to/jar_file1:/path/to/jar_file2 spark.executor.extraClassPath To add Arrow Data Source and Native SQL Engine jar file in Spark Executor /path/to/jar_file1:/path/to/jar_file2 spark.executorEnv.LIBARROW_DIR To set up the location of Arrow library, by default it will search the loation of jar to be uncompressed /path/to/arrow_library/ spark.executorEnv.CC To set up the location of gcc /path/to/gcc/ spark.executor.memory To set up how much memory to be used for Spark Executor. spark.memory.offHeap.size To set up how much memory to be used for Java OffHeap. Please notice Native SQL Engine will leverage this setting to allocate memory space for native usage even offHeap is disabled. The value is based on your system and it is recommended to set it larger if you are facing Out of Memory issue in Native SQL Engine 30G spark.executor.extraJavaOptions To set up how much Direct Memory to be used for Native SQL Engine. The value is based on your system and it is recommended to set it larger if you are facing Out of Memory issue in Native SQL Engine -XX:MaxDirectMemorySize=30G spark.sql.sources.useV1SourceList Choose to use V1 source avro spark.sql.join.preferSortMergeJoin To turn off preferSortMergeJoin in Spark false spark.sql.extensions To turn on Native SQL Engine Plugin com.intel.oap.ColumnarPlugin spark.shuffle.manager To turn on Native SQL Engine Columnar Shuffle Plugin org.apache.spark.shuffle.sort.ColumnarShuffleManager spark.oap.sql.columnar.batchscan Enable or Disable Columnar Batchscan, default is true true spark.oap.sql.columnar.hashagg Enable or Disable Columnar Hash Aggregate, default is true true spark.oap.sql.columnar.projfilter Enable or Disable Columnar Project and Filter, default is true true spark.oap.sql.columnar.codegen.sort Enable or Disable Columnar Sort, default is true true spark.oap.sql.columnar.window Enable or Disable Columnar Window, default is true true spark.oap.sql.columnar.shuffledhashjoin Enable or Disable ShffuledHashJoin, default is true true spark.oap.sql.columnar.sortmergejoin Enable or Disable Columnar Sort Merge Join, default is true true spark.oap.sql.columnar.union Enable or Disable Columnar Union, default is true true spark.oap.sql.columnar.expand Enable or Disable Columnar Expand, default is true true spark.oap.sql.columnar.broadcastexchange Enable or Disable Columnar Broadcast Exchange, default is true true spark.oap.sql.columnar.nanCheck Enable or Disable Nan Check, default is true true spark.oap.sql.columnar.hashCompare Enable or Disable Hash Compare in HashJoins or HashAgg, default is true true spark.oap.sql.columnar.broadcastJoin Enable or Disable Columnar BradcastHashJoin, default is true true spark.oap.sql.columnar.wholestagecodegen Enable or Disable Columnar WholeStageCodeGen, default is true true spark.oap.sql.columnar.preferColumnar Enable or Disable Columnar Operators, default is false. This parameter could impact the performance in different case. In some cases, to set false can get some performance boost. false spark.oap.sql.columnar.joinOptimizationLevel Fallback to row operators if there are several continous joins 6 spark.sql.execution.arrow.maxRecordsPerBatch Set up the Max Records per Batch 10000 spark.oap.sql.columnar.wholestagecodegen.breakdownTime Enable or Disable metrics in Columnar WholeStageCodeGen false spark.oap.sql.columnar.tmp_dir Set up a folder to store the codegen files /tmp spark.oap.sql.columnar.shuffle.customizedCompression.codec Set up the codec to be used for Columnar Shuffle, default is lz4 lz4 spark.oap.sql.columnar.numaBinding Set up NUMABinding, default is false true spark.oap.sql.columnar.coreRange Set up the core range for NUMABinding, only works when numaBinding set to true. The setting is based on the number of cores in your system. Use 72 cores as an example. 0-17,36-53 |18-35,54-71 Below is an example for spark-default.conf, if you are using conda to install OAP project. ##### Columnar Process Configuration spark.sql.sources.useV1SourceList avro spark.sql.join.preferSortMergeJoin false spark.sql.extensions com.intel.oap.ColumnarPlugin spark.shuffle.manager org.apache.spark.shuffle.sort.ColumnarShuffleManager # note native sql engine depends on arrow data source spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executorEnv.LIBARROW_DIR $HOME/miniconda2/envs/oapenv spark.executorEnv.CC $HOME/miniconda2/envs/oapenv/bin/gcc ###### Before you start spark, you must use below command to add some environment variables. export CC=$HOME/miniconda2/envs/oapenv/bin/gcc export LIBARROW_DIR=$HOME/miniconda2/envs/oapenv/","title":"Spark Configurations for Native SQL Engine"},{"location":"Installation/","text":"Spark Native SQL Engine Installation For detailed testing scripts, please refer to solution guide Install Googletest and Googlemock yum install gtest-devel yum install gmock Build Native SQL Engine git clone -b ${version} https://github.com/oap-project/native-sql-engine.git cd oap-native-sql mvn clean package -DskipTests -Dcpp_tests=OFF -Dbuild_arrow=ON -Dcheckstyle.skip Based on the different environment, there are some parameters can be set via -D with mvn. Parameters Description Default Value cpp_tests Enable or Disable CPP Tests False build_arrow Build Arrow from Source True arrow_root When build_arrow set to False, arrow_root will be enabled to find the location of your existing arrow library. /usr/local build_protobuf Build Protobuf from Source. If set to False, default library path will be used to find protobuf library. True When build_arrow set to True, the build_arrow.sh will be launched and compile a custom arrow library from OAP Arrow If you wish to change any parameters from Arrow, you can change it from the build_arrow.sh script under native-sql-enge/arrow-data-source/script/. Additional Notes Notes for Installation Issues","title":"Installation"},{"location":"Installation/#spark-native-sql-engine-installation","text":"For detailed testing scripts, please refer to solution guide","title":"Spark Native SQL Engine Installation"},{"location":"Installation/#install-googletest-and-googlemock","text":"yum install gtest-devel yum install gmock","title":"Install Googletest and Googlemock"},{"location":"Installation/#build-native-sql-engine","text":"git clone -b ${version} https://github.com/oap-project/native-sql-engine.git cd oap-native-sql mvn clean package -DskipTests -Dcpp_tests=OFF -Dbuild_arrow=ON -Dcheckstyle.skip Based on the different environment, there are some parameters can be set via -D with mvn. Parameters Description Default Value cpp_tests Enable or Disable CPP Tests False build_arrow Build Arrow from Source True arrow_root When build_arrow set to False, arrow_root will be enabled to find the location of your existing arrow library. /usr/local build_protobuf Build Protobuf from Source. If set to False, default library path will be used to find protobuf library. True When build_arrow set to True, the build_arrow.sh will be launched and compile a custom arrow library from OAP Arrow If you wish to change any parameters from Arrow, you can change it from the build_arrow.sh script under native-sql-enge/arrow-data-source/script/.","title":"Build Native SQL Engine"},{"location":"Installation/#additional-notes","text":"Notes for Installation Issues","title":"Additional Notes"},{"location":"InstallationNotes/","text":"Notes for Installation Issues Before the Installation, if you have installed other version of oap-native-sql, remove all installed lib and include from system path: libarrow libgandiva libspark-columnar-jni* libgandiva_jni.so was not found inside JAR change property 'arrow.cpp.build.dir' to $ARROW_DIR/cpp/release-build/release/ in gandiva/pom.xml. If you do not want to change the contents of pom.xml, specify it like this: mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=/root/git/t/arrow/cpp/release-build/release/ -DskipTests -Dcheckstyle.skip No rule to make target '../src/protobuf_ep', needed by `src/proto/Exprs.pb.cc' remove the existing libprotobuf installation, then the script for find_package() will be able to download protobuf. can't find the libprotobuf.so.13 in the shared lib copy the libprotobuf.so.13 from $OAP_DIR/oap-native-sql/cpp/src/resources to /usr/lib64/ unable to load libhdfs: libgsasl.so.7: cannot open shared object file libgsasl is missing, run yum install libgsasl CentOS 7.7 looks like didn't provide the glibc we required, so binaries packaged on F30 won't work. 20/04/21 17:46:17 WARN TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, 10.0.0.143, executor 6): java.lang.UnsatisfiedLinkError: /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336) Missing symbols due to old GCC version. [root@vsr243 release-build]# nm /usr/local/lib64/libparquet.so | grep ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE _ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE Need to compile all packags with newer GCC: [root@vsr243 ~]# export CXX=/usr/local/bin/g++ [root@vsr243 ~]# export CC=/usr/local/bin/gcc Can not connect to hdfs @sr602 vsr606, vsr243 are both not able to connect to hdfs @sr602, need to skipTests to generate the jar","title":"InstallationNotes"},{"location":"InstallationNotes/#notes-for-installation-issues","text":"Before the Installation, if you have installed other version of oap-native-sql, remove all installed lib and include from system path: libarrow libgandiva libspark-columnar-jni* libgandiva_jni.so was not found inside JAR change property 'arrow.cpp.build.dir' to $ARROW_DIR/cpp/release-build/release/ in gandiva/pom.xml. If you do not want to change the contents of pom.xml, specify it like this: mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=/root/git/t/arrow/cpp/release-build/release/ -DskipTests -Dcheckstyle.skip No rule to make target '../src/protobuf_ep', needed by `src/proto/Exprs.pb.cc' remove the existing libprotobuf installation, then the script for find_package() will be able to download protobuf. can't find the libprotobuf.so.13 in the shared lib copy the libprotobuf.so.13 from $OAP_DIR/oap-native-sql/cpp/src/resources to /usr/lib64/ unable to load libhdfs: libgsasl.so.7: cannot open shared object file libgsasl is missing, run yum install libgsasl CentOS 7.7 looks like didn't provide the glibc we required, so binaries packaged on F30 won't work. 20/04/21 17:46:17 WARN TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, 10.0.0.143, executor 6): java.lang.UnsatisfiedLinkError: /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336) Missing symbols due to old GCC version. [root@vsr243 release-build]# nm /usr/local/lib64/libparquet.so | grep ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE _ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE Need to compile all packags with newer GCC: [root@vsr243 ~]# export CXX=/usr/local/bin/g++ [root@vsr243 ~]# export CC=/usr/local/bin/gcc Can not connect to hdfs @sr602 vsr606, vsr243 are both not able to connect to hdfs @sr602, need to skipTests to generate the jar","title":"Notes for Installation Issues"},{"location":"OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Native SQL Engine Building OAP Prerequisites We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.1.0-spark-3.0.0 corresponds to all OAP modules' tag version v1.1.0-spark-3.0.0 . Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. Building OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Native SQL Engine","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"OAP-Developer-Guide/#prerequisites","text":"We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.1.0-spark-3.0.0 corresponds to all OAP modules' tag version v1.1.0-spark-3.0.0 . Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance.","title":"Prerequisites"},{"location":"OAP-Developer-Guide/#building","text":"OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, reload your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.1.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, reload your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"OAP-Installation-Guide/#installing-oap","text":"Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.1.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI","title":"Installing OAP"},{"location":"OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"Prerequisite/","text":"Prerequisite There are some requirements before you build the project. Please make sure you have already installed the software in your system. GCC 7.0 or higher version LLVM 7.0 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.16 or higher version Maven 3.6.3 or higher version Hadoop 2.7.5 or higher version Spark 3.0.0 or higher version Intel Optimized Arrow 3.0.0 gcc installation // installing GCC 7.0 or higher version Please notes for better performance support, GCC 7.0 is a minimal requirement with Intel Microarchitecture such as SKYLAKE, CASCADELAKE, ICELAKE. https://gcc.gnu.org/install/index.html Follow the above website to download gcc. C++ library may ask a certain version, if you are using GCC 7.0 the version would be libstdc++.so.6.0.28. You may have to launch ./contrib/download_prerequisites command to install all the prerequisites for gcc. If you are facing downloading issue in download_prerequisites command, you can try to change ftp to http. //Follow the steps to configure gcc https://gcc.gnu.org/install/configure.html If you are facing a multilib issue, you can try to add --disable-multilib parameter in ../configure //Follow the steps to build gc https://gcc.gnu.org/install/build.html //Follow the steps to install gcc https://gcc.gnu.org/install/finalinstall.html //Set up Environment for new gcc export PATH=$YOUR_GCC_INSTALLATION_DIR/bin:$PATH export LD_LIBRARY_PATH=$YOUR_GCC_INSTALLATION_DIR/lib64:$LD_LIBRARY_PATH Please remember to add and source the setup in your environment files such as /etc/profile or /etc/bashrc //Verify if gcc has been installation Use gcc -v command to verify if your gcc version is correct.(Must larger than 7.0) LLVM 7.0 installation Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install cmake installation If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing Cmake 3.16.1 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake maven installation If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget htps://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn HADOOP/SPARK Installation If there is no existing Hadoop/Spark installed, Please follow the guide to install your Hadoop/Spark SPARK/HADOOP Installation Hadoop Native Library(Default) Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation Use libhdfs3 library for better performance(Optional) For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so We also provide a libhdfs3 binary in cpp/src/resources directory. To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto. Intel Optimized Apache Arrow Installation During the mvn compile command, it will launch a script(build_arrow.sh) to help install and compile a Intel custom Arrow library. If you wish to build Apache Arrow by yourself, please follow the guide to build and install Apache Arrow ArrowInstallation","title":"Prerequisite"},{"location":"Prerequisite/#prerequisite","text":"There are some requirements before you build the project. Please make sure you have already installed the software in your system. GCC 7.0 or higher version LLVM 7.0 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.16 or higher version Maven 3.6.3 or higher version Hadoop 2.7.5 or higher version Spark 3.0.0 or higher version Intel Optimized Arrow 3.0.0","title":"Prerequisite"},{"location":"Prerequisite/#gcc-installation","text":"// installing GCC 7.0 or higher version Please notes for better performance support, GCC 7.0 is a minimal requirement with Intel Microarchitecture such as SKYLAKE, CASCADELAKE, ICELAKE. https://gcc.gnu.org/install/index.html Follow the above website to download gcc. C++ library may ask a certain version, if you are using GCC 7.0 the version would be libstdc++.so.6.0.28. You may have to launch ./contrib/download_prerequisites command to install all the prerequisites for gcc. If you are facing downloading issue in download_prerequisites command, you can try to change ftp to http. //Follow the steps to configure gcc https://gcc.gnu.org/install/configure.html If you are facing a multilib issue, you can try to add --disable-multilib parameter in ../configure //Follow the steps to build gc https://gcc.gnu.org/install/build.html //Follow the steps to install gcc https://gcc.gnu.org/install/finalinstall.html //Set up Environment for new gcc export PATH=$YOUR_GCC_INSTALLATION_DIR/bin:$PATH export LD_LIBRARY_PATH=$YOUR_GCC_INSTALLATION_DIR/lib64:$LD_LIBRARY_PATH Please remember to add and source the setup in your environment files such as /etc/profile or /etc/bashrc //Verify if gcc has been installation Use gcc -v command to verify if your gcc version is correct.(Must larger than 7.0)","title":"gcc installation"},{"location":"Prerequisite/#llvm-70-installation","text":"Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install","title":"LLVM 7.0 installation"},{"location":"Prerequisite/#cmake-installation","text":"If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing Cmake 3.16.1 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake","title":"cmake installation"},{"location":"Prerequisite/#maven-installation","text":"If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget htps://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn","title":"maven installation"},{"location":"Prerequisite/#hadoopspark-installation","text":"If there is no existing Hadoop/Spark installed, Please follow the guide to install your Hadoop/Spark SPARK/HADOOP Installation","title":"HADOOP/SPARK Installation"},{"location":"Prerequisite/#hadoop-native-librarydefault","text":"Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation","title":"Hadoop Native Library(Default)"},{"location":"Prerequisite/#use-libhdfs3-library-for-better-performanceoptional","text":"For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so We also provide a libhdfs3 binary in cpp/src/resources directory. To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto.","title":"Use libhdfs3 library for better performance(Optional)"},{"location":"Prerequisite/#intel-optimized-apache-arrow-installation","text":"During the mvn compile command, it will launch a script(build_arrow.sh) to help install and compile a Intel custom Arrow library. If you wish to build Apache Arrow by yourself, please follow the guide to build and install Apache Arrow ArrowInstallation","title":"Intel Optimized Apache Arrow Installation"},{"location":"SparkInstallation/","text":"Download Spark 3.0.1 Currently Native SQL Engine works on the Spark 3.0.1 version. wget http://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz sudo mkdir -p /opt/spark && sudo mv spark-3.0.1-bin-hadoop3.2.tgz /opt/spark sudo cd /opt/spark && sudo tar -xf spark-3.0.1-bin-hadoop3.2.tgz export SPARK_HOME=/opt/spark/spark-3.0.1-bin-hadoop3.2/ Or building Spark from source git clone https://github.com/intel-bigdata/spark.git cd spark && git checkout native-sql-engine-clean # check spark supported hadoop version grep \\<hadoop\\.version\\> -r pom.xml <hadoop.version>2.7.4</hadoop.version> <hadoop.version>3.2.0</hadoop.version> # so we should build spark specifying hadoop version as 3.2 ./build/mvn -Pyarn -Phadoop-3.2 -Dhadoop.version=3.2.0 -DskipTests clean install Specify SPARK_HOME to spark path export SPARK_HOME=${HADOOP_PATH} Hadoop building from source git clone https://github.com/apache/hadoop.git cd hadoop git checkout rel/release-3.2.0 # only build binary for hadoop mvn clean install -Pdist -DskipTests -Dtar # build binary and native library such as libhdfs.so for hadoop # mvn clean install -Pdist,native -DskipTests -Dtar export HADOOP_HOME=${HADOOP_PATH}/hadoop-dist/target/hadoop-3.2.0/","title":"SparkInstallation"},{"location":"SparkInstallation/#download-spark-301","text":"Currently Native SQL Engine works on the Spark 3.0.1 version. wget http://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz sudo mkdir -p /opt/spark && sudo mv spark-3.0.1-bin-hadoop3.2.tgz /opt/spark sudo cd /opt/spark && sudo tar -xf spark-3.0.1-bin-hadoop3.2.tgz export SPARK_HOME=/opt/spark/spark-3.0.1-bin-hadoop3.2/","title":"Download Spark 3.0.1"},{"location":"SparkInstallation/#or-building-spark-from-source","text":"git clone https://github.com/intel-bigdata/spark.git cd spark && git checkout native-sql-engine-clean # check spark supported hadoop version grep \\<hadoop\\.version\\> -r pom.xml <hadoop.version>2.7.4</hadoop.version> <hadoop.version>3.2.0</hadoop.version> # so we should build spark specifying hadoop version as 3.2 ./build/mvn -Pyarn -Phadoop-3.2 -Dhadoop.version=3.2.0 -DskipTests clean install Specify SPARK_HOME to spark path export SPARK_HOME=${HADOOP_PATH}","title":"Or building Spark from source"},{"location":"SparkInstallation/#hadoop-building-from-source","text":"git clone https://github.com/apache/hadoop.git cd hadoop git checkout rel/release-3.2.0 # only build binary for hadoop mvn clean install -Pdist -DskipTests -Dtar # build binary and native library such as libhdfs.so for hadoop # mvn clean install -Pdist,native -DskipTests -Dtar export HADOOP_HOME=${HADOOP_PATH}/hadoop-dist/target/hadoop-3.2.0/","title":"Hadoop building from source"},{"location":"User-Guide/","text":"Spark Native SQL Engine A Native Engine for Spark SQL with vectorized SIMD optimizations Introduction Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD optimized kernels and LLVM based SQL engine Gandiva are also very efficient. Native SQL Engine used these technologies and brought better performance to Spark SQL. Key Features Apache Arrow formatted intermediate data among Spark operator With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format. Apache Arrow based Native Readers for Parquet and other formats A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source Apache Arrow Compute/Gandiva based operators We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch. Native Columnar Shuffle Operator with efficient compression support We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format. Please check the operator supporting details here Build the Plugin Building by Conda If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-columnar-core-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Get Started . Building by yourself If you prefer to build from the source code on your hand, please follow below steps to set up your environment. Prerequisite There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. If you are running a SPARK Cluster, please make sure all the software are installed in every single node. Installation Please check the document Installation Guide Configuration & Testing Please check the document Configuration Guide Get started To enable OAP NativeSQL Engine, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar should be added to Spark configuration. We also recommend to use spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar . We will demonstrate an example by using both jar files. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if native sql engine works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/date_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should show up on Spark console and you can check the DAG diagram with some Columnar Processing stage. Native SQL engine still lacks some features, please check out the limitations . Performance data For advanced performance testing, below charts show the results by using two benchmarks: 1. Decision Support Benchmark1 and 2. Decision Support Benchmark2. All the testing environment for Decision Support Benchmark1&2 are using 1 master + 3 workers and Intel(r) Xeon(r) Gold 6252 CPU|384GB memory|NVMe SSD x3 per single node with 1.5TB dataset. * Decision Support Benchmark1 is a query set modified from TPC-H benchmark . We change Decimal to Double since Decimal hasn't been supported in OAP v1.0-Native SQL Engine. Overall, the result shows a 1.49X performance speed up from OAP v1.0-Native SQL Engine comparing to Vanilla SPARK 3.0.0. We also put the detail result by queries, most of queries in Decision Support Benchmark1 can take the advantages from Native SQL Engine. The performance boost ratio may depend on the individual query. Decision Support Benchmark2 is a query set modified from TPC-DS benchmark . We change Decimal to Doubel since Decimal hasn't been supported in OAP v1.0-Native SQL Engine. We pick up 10 queries which can be fully supported in OAP v1.0-Native SQL Engine and the result shows a 1.26X performance speed up comparing to Vanilla SPARK 3.0.0. Please notes the performance data is not an official from TPC-H and TPC-DS. The actual performance result may vary by individual workloads. Please try your workloads with native SQL Engine first and check the DAG or log file to see if all the operators can be supported in OAP-Native SQL Engine. Coding Style For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details. Contact chendi.xue@intel.com binwei.yang@intel.com","title":"User Guide"},{"location":"User-Guide/#spark-native-sql-engine","text":"A Native Engine for Spark SQL with vectorized SIMD optimizations","title":"Spark Native SQL Engine"},{"location":"User-Guide/#introduction","text":"Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD optimized kernels and LLVM based SQL engine Gandiva are also very efficient. Native SQL Engine used these technologies and brought better performance to Spark SQL.","title":"Introduction"},{"location":"User-Guide/#key-features","text":"","title":"Key Features"},{"location":"User-Guide/#apache-arrow-formatted-intermediate-data-among-spark-operator","text":"With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format.","title":"Apache Arrow formatted intermediate data among Spark operator"},{"location":"User-Guide/#apache-arrow-based-native-readers-for-parquet-and-other-formats","text":"A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source","title":"Apache Arrow based Native Readers for Parquet and other formats"},{"location":"User-Guide/#apache-arrow-computegandiva-based-operators","text":"We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch.","title":"Apache Arrow Compute/Gandiva based operators"},{"location":"User-Guide/#native-columnar-shuffle-operator-with-efficient-compression-support","text":"We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format. Please check the operator supporting details here","title":"Native Columnar Shuffle Operator with efficient compression support"},{"location":"User-Guide/#build-the-plugin","text":"","title":"Build the Plugin"},{"location":"User-Guide/#building-by-conda","text":"If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-columnar-core-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Get Started .","title":"Building by Conda"},{"location":"User-Guide/#building-by-yourself","text":"If you prefer to build from the source code on your hand, please follow below steps to set up your environment.","title":"Building by yourself"},{"location":"User-Guide/#prerequisite","text":"There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. If you are running a SPARK Cluster, please make sure all the software are installed in every single node.","title":"Prerequisite"},{"location":"User-Guide/#installation","text":"Please check the document Installation Guide","title":"Installation"},{"location":"User-Guide/#configuration-testing","text":"Please check the document Configuration Guide","title":"Configuration &amp; Testing"},{"location":"User-Guide/#get-started","text":"To enable OAP NativeSQL Engine, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar should be added to Spark configuration. We also recommend to use spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar . We will demonstrate an example by using both jar files. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if native sql engine works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/date_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should show up on Spark console and you can check the DAG diagram with some Columnar Processing stage. Native SQL engine still lacks some features, please check out the limitations .","title":"Get started"},{"location":"User-Guide/#performance-data","text":"For advanced performance testing, below charts show the results by using two benchmarks: 1. Decision Support Benchmark1 and 2. Decision Support Benchmark2. All the testing environment for Decision Support Benchmark1&2 are using 1 master + 3 workers and Intel(r) Xeon(r) Gold 6252 CPU|384GB memory|NVMe SSD x3 per single node with 1.5TB dataset. * Decision Support Benchmark1 is a query set modified from TPC-H benchmark . We change Decimal to Double since Decimal hasn't been supported in OAP v1.0-Native SQL Engine. Overall, the result shows a 1.49X performance speed up from OAP v1.0-Native SQL Engine comparing to Vanilla SPARK 3.0.0. We also put the detail result by queries, most of queries in Decision Support Benchmark1 can take the advantages from Native SQL Engine. The performance boost ratio may depend on the individual query. Decision Support Benchmark2 is a query set modified from TPC-DS benchmark . We change Decimal to Doubel since Decimal hasn't been supported in OAP v1.0-Native SQL Engine. We pick up 10 queries which can be fully supported in OAP v1.0-Native SQL Engine and the result shows a 1.26X performance speed up comparing to Vanilla SPARK 3.0.0. Please notes the performance data is not an official from TPC-H and TPC-DS. The actual performance result may vary by individual workloads. Please try your workloads with native SQL Engine first and check the DAG or log file to see if all the operators can be supported in OAP-Native SQL Engine.","title":"Performance data"},{"location":"User-Guide/#coding-style","text":"For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details.","title":"Coding Style"},{"location":"User-Guide/#contact","text":"chendi.xue@intel.com binwei.yang@intel.com","title":"Contact"},{"location":"limitations/","text":"Limitations for Native SQL Engine Spark compability Native SQL engine currenlty works with Spark 3.0.0 only. There are still some trouble with latest Shuffle/AQE API from Spark 3.0.1, 3.0.2 or 3.1.x. Operator limitations All performance critical operators in TPC-H/TPC-DS should be supported. For those unsupported operators, Native SQL engine will automatically fallback to row operators in vanilla Spark. Columnar Projection with Filter We used 16 bit selection vector for filter so the max batch size need to be < 65536 Columnar Sort Columnar Sort does not support spill to disk yet. To reduce the peak memory usage, we used smaller data structure(uin16_t), so this limits - the max batch size to be < 65536 - the number of batches in one partiton to be < 65536","title":"Limitations for Native SQL Engine"},{"location":"limitations/#limitations-for-native-sql-engine","text":"","title":"Limitations for Native SQL Engine"},{"location":"limitations/#spark-compability","text":"Native SQL engine currenlty works with Spark 3.0.0 only. There are still some trouble with latest Shuffle/AQE API from Spark 3.0.1, 3.0.2 or 3.1.x.","title":"Spark compability"},{"location":"limitations/#operator-limitations","text":"All performance critical operators in TPC-H/TPC-DS should be supported. For those unsupported operators, Native SQL engine will automatically fallback to row operators in vanilla Spark.","title":"Operator limitations"},{"location":"limitations/#columnar-projection-with-filter","text":"We used 16 bit selection vector for filter so the max batch size need to be < 65536","title":"Columnar Projection with Filter"},{"location":"limitations/#columnar-sort","text":"Columnar Sort does not support spill to disk yet. To reduce the peak memory usage, we used smaller data structure(uin16_t), so this limits - the max batch size to be < 65536 - the number of batches in one partiton to be < 65536","title":"Columnar Sort"},{"location":"operators/","text":"No. Executor Description Notes BOOLEAN BYTE SHORT INT LONG FLOAT DOUBLE STRING DECIMAL DATE TIMESTAMP NULL BINARY CALENDAR ARRAY MAP STRUCT UDT 1 CoalesceExec y y y y y y y y y y 2 CollectLimitExec using row version 3 ExpandExec y y y y y y y y y y 4 FileSourceScanExec y y y y y y y y y y 5 FilterExec y y y y y y y y y y 6 GenerateExec using row version 7 GlobalLimitExec using row version 8 LocalLimitExec 9 ProjectExec y y y y y y y y y y 10 RangeExec using row version 11 SortExec y y y y y y y y y y 12 TakeOrderedAndPorjectExec using row version 13 UnionExec y y y y y y y y y y 14 CustomShuffleReaderExec y y y y y y y y y y 15 HashAggregateExec y y y y y y y y y y 16 SortAggregateExec y y y y y y y y y y 17 DataWritingCommandExec using row version 18 BatchScanExec y y y y y y y y y y 19 BroadcastExchangeExec y y y y y y y y y y 20 ShuffleExchangeExec y y y y y y y y y y 21 BroadcastHashJoinExec y y y y y y y y y y 22 BroadcastNestedLoopJoinExec using row version 23 CartesianProductExec using row version 24 ShuffledHashJoinExec y y y y y y y y y y 25 SortMergeJoinExec y y y y y y y y y y 26 ArrowEvalPythonExec using row version 27 WindowINPandasExec using row version 28 WindowExec y y y y y y y y y y","title":"Operators"}]}